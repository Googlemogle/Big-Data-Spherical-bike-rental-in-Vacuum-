{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data - Весна 2024 \"Сферический Велопрокат в Вакууме\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Учащимся выдается датасет с многолетними накопленными историческими\n",
    ">данными о поездках велопроката Divvy Bikes. На основании этого датасета\n",
    ">нужно оценить объем рынка и характер его динамики, а затем разработать\n",
    ">модель юнит-экономики поездки для различных сценариев использования\n",
    ">сервиса: разовая покупка, месячная подписка и т.д.\n",
    ">Тезисный план:\n",
    ">1. Разведочный анализ датасета\n",
    ">2. Подготовка аналитических витрин данных\n",
    ">3. Расширенный анализ витрин с целью оценки объема рынка, характера\n",
    ">динамики и прогнозирования роста\n",
    ">4. Разработка модели юнит-экономики поездки\n",
    ">5. Анализ чувствительности по основным параметрам модели\n",
    ">6. Визуализация ключевых показателей\n",
    ">7. Подготовка рассказа, презентации и дашборда с основными\n",
    "результатами и выводами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт Библиотек и загрузка датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gosha\\AppData\\Local\\Temp\\ipykernel_10748\\3043270857.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency, ttest_ind, mannwhitneyu, shapiro, permutation_test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import re\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2020\n",
    "df = pd.read_csv(f\"{x}-cleared-divvy-tripdata.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data_df: pd.DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "        Функция для просмотра сводной информации о датасете\n",
    "    \"\"\"\n",
    "    print ('\\033[1m' + 'Сводная информация об исходных данных'+ '\\033[0m')\n",
    "    print(data_df.info())\n",
    "    print(data_df.shape)\n",
    "        \n",
    "    missed_cells = data_df.isnull().sum().sum()/(data_df.shape[0]*(data_df.shape[1]-1)) # пропущенные ячейки\n",
    "    missed_rows = sum(data_df.isnull().sum(axis = 1)>0)/data_df.shape[0]\n",
    "    print ('\\033[1m' + '\\nПроверка пропусков'+ '\\033[0m')\n",
    "    print ('Количество пропусков: {:.0f}'.format(data_df.isnull().sum().sum()))\n",
    "    print ('Доля пропусков: {:.1%}'.format(missed_cells)+ '\\033[0m')\n",
    "    print ('Доля строк, содержащих пропуски: {:.1%}'.format(missed_rows))\n",
    "\n",
    "    ## Проверим дубликаты\n",
    "    print ('\\033[1m' + '\\nПроверка на дубликаты'+ '\\033[0m')\n",
    "    print('Количество полных дубликатов: ', data_df.duplicated().sum())\n",
    "        \n",
    "    ## Посмотрим на сами данные\n",
    "    print ('\\033[1m' + '\\nПервые десять строк датасета'+ '\\033[0m')\n",
    "    display(data_df.head(10))\n",
    "    \n",
    "    print('\\033[1m' + '\\nОписание количественных данных:'+ '\\033[0m')\n",
    "    display(data_df.describe().T)\n",
    "    \n",
    "    print('\\033[1m' + '\\nОписание категориальных данных:'+ '\\033[0m')\n",
    "    display(data_df.describe(include='object').T) \n",
    "    \n",
    "    \n",
    "    print('\\033[1m' + '\\nВывод уникальных значений по каждому категориальному признаку без учета id:'+ '\\033[0m')    \n",
    "    df_object = data_df.select_dtypes(include='object').columns\n",
    "    \n",
    "    for i in df_object:\n",
    "        if data_df[i].nunique() > 10:\n",
    "            continue\n",
    "        print('\\033[1m' + ('_' * 10) + '\\033[0m')\n",
    "        display(data_df[i].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, col_column):\n",
    "    '''\n",
    "    Функция отрисовки гистограмм и ящика с усами для количественных переменных.\n",
    "    На вход: исходная таблица и список количественных переменных.\n",
    "    На выходе: графики\n",
    "    '''\n",
    "    rows = len(col_column)\n",
    "    f, ax = plt.subplots(rows, 2, figsize=(8, 16))\n",
    "    f.tight_layout()\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(8)\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    for i, col in enumerate(col_column):         \n",
    "        sns.histplot(data[col], kde=True, bins=16, ax = ax[i, 0])                    \n",
    "        sns.boxplot(data[col], ax = ax[i, 1])\n",
    "\n",
    "        ax[i, 0].set_xlabel(col)\n",
    "        ax[i, 1].set_xlabel(col)\n",
    "        ax[i, 0].set_ylabel('Количество')\n",
    "        ax[i, 1].set_ylabel(\"\")\n",
    "    plt.suptitle(\"Гистограмма и ящик с усами для количественных данных\", fontsize=22, y=1.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(df, [\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Категориальные столбцы с фиксированными возможными значениями проверены** в функции и дальнейшей обработки не потребуют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важное замечание: мы не будем отбрасывать выбросы с помощью с помощью перцентилей в столбце со временем поездки так как столбец имеет экспоненциалльное распределение, но и не учитывать его (то-есть перейти к более большим величинам мы не можем потому что большее число клиентов пользуются велосипедами на короткий промежуток времени (меньше часа)) мы не можем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ride_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уникальный идентификатор каждой поездки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"ride_id\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### started_at и ended_at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**started_at** : дата и время начала поездки  \n",
    "**ended_at** : дата и время окончания поездки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Исправляю формат данных столбцов started_at и ended_at на дату\n",
    "df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "df[\"ended_at\"] = pd.to_datetime(df[\"ended_at\"])\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ride_duration\"] = (df[\"ended_at\"] - df[\"started_at\"]) # столбец содержащий время поездки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"ride_duration\"] < pd.Timedelta(days=1)]\n",
    "# sns.histplot(df[\"ride_duration\"].apply(lambda x: x.total_seconds()/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете имеются ошибки с датой начала и конца поездки, а также хоть и на сайте написано что данные предобрабатываются от случайного срабатывания (человек пытался переустановить велосипед на стоянку чтобы убедиться в его безопасности или это может быть ложный старт) по факту в датасете их полным полно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "bigger = df[df[\"started_at\"] > df[\"ended_at\"]]\n",
    "lower_1m = df[df[\"ride_duration\"] < pd.Timedelta(minutes=1)]\n",
    "df = df[df[\"started_at\"] < df[\"ended_at\"]]\n",
    "df[\"fix\"] = df[\"end_station_name\"].apply(lambda x: x in (\"HUBBARD ST BIKE CHECKING (LBS-WH-TEST)\", \n",
    "                                                         \"Base - 2132 W Hubbard Warehouse\",\n",
    "                                                         \"hubbard_test_lws\", \n",
    "                                                         \"WATSON TESTING - DIVVY\")) + \\\n",
    "            df[\"start_station_name\"].apply(lambda x: x in (\"HUBBARD ST BIKE CHECKING (LBS-WH-TEST)\", \n",
    "                                                           \"Base - 2132 W Hubbard Warehouse\",\n",
    "                                                           \"hubbard_test_lws\", \n",
    "                                                           \"WATSON TESTING - DIVVY\"))\n",
    "\n",
    "fix_df = df[df[\"fix\"]]\n",
    "print(fix_df.shape)\n",
    "df = df[df[\"fix\"] == False]\n",
    "\n",
    "day = df[df[\"ride_duration\"] > pd.Timedelta(days=1)] # больше часа на велосипеде катаются всего 5000 записей из 300 тысяч\n",
    "print(day.shape)\n",
    "print(df.sort_values(by=\"ride_duration\", ascending=False).shape)\n",
    "print(df.sort_values(by=\"ride_duration\").iloc[-1])\n",
    "\n",
    "df = df[df[\"ride_duration\"] > pd.Timedelta(minutes=1)]\n",
    "df = df[df[\"ride_duration\"] < pd.Timedelta(days=1)]\n",
    "print(df.shape[0])\n",
    "print(df[\"ride_duration\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы удаляем значения с длительностью поездки больше дня так как таких значений всего около 2500 тысяч и нам не так важны эти кейсы при анализе рынка  \n",
    "Хотя можно попробовать проаанализировать эти данные отедльно и сказать что то по ним"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимум длительности поездки 108 дней  \n",
    "Точка конца поездки подозрительно близко находится с не очень людным местом около берега  \n",
    "Могу предположить что велосипед бросили где то около моря после чего его кто то нашел и поставил на станцию через 4 месяца  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметил подозрительные значения HUBBARD ST BIKE CHECKING (LBS-WH-TEST) и узнал что это рементные станции которые divvy использует для ремонта велосипедов  \n",
    "Мы должны удалить из датасета значения с точками старта и конца в следующих точках:\n",
    "- Base - 2132 W Hubbard Warehouse  \n",
    "- HUBBARD ST BIKE CHECKING (LBS-WH-TEST)  \n",
    "- hubbard_test_lws  \n",
    "- WATSON TESTING - DIVVY\n",
    "\n",
    "**Поездок совершенных сотрудниками для осуществления ремонта оказалось 509**\n",
    "Схораним это в отдельный датасет чтобы использовать потом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"ride_duration\"].apply(lambda x: x.total_seconds()/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day\"] = df[\"started_at\"].apply(lambda x: x.day)\n",
    "df[\"month\"] = df[\"started_at\"].apply(lambda x: x.month)\n",
    "df[\"year\"] = df[\"started_at\"].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"].unique().size > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"].unique().size > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Названия и id точки начала поездки и конца"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start_station_name: название станции, с которой началось путешествие\n",
    "- start_station_id: уникальный идентификатор станции, с которой началась поездка\n",
    "- end_station_name: название станции, на которой закончилась поездка\n",
    "- end_station_id: уникальный идентификатор станции, на которой закончилась поездка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"start_station_name\", \"start_station_id\", \"end_station_name\", \"end_station_id\"]]\n",
    "print(data.isna().sum())\n",
    "missed_cells = data.isna().sum().sum()/(data.shape[0]*(data.shape[1]))\n",
    "print ('Доля пропусков: {:.1%}'.format(missed_cells)+ '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При исследовании данных столбцов было замечено **очень много пропусков** (15% от объема данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посторили график и обнаружили зависимость между отсутствующими значениями и типом арендуемого велосипеда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day = sorted(df[\"day\"].unique())[-1] # переменная содержащая цифру по счету максимального дня в месяце\n",
    "\n",
    "# print(df[[\"start_station_name\", \"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]].groupby([\"start_station_name\"]).head(2))\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "sns.histplot(df[\"day\"], kde=True, bins=max_day, ax=ax[0, 0])\n",
    "ax[0, 0].set_ylabel(\"Аренда велосипедов по дням\")\n",
    "sns.histplot(df[\"rideable_type\"], ax=ax[1, 0])\n",
    "ax[1, 0].set_ylabel(\"Доля электро и обычных велосипедов\")\n",
    "ax[0, 0].set_title(\"До\", fontsize=14)\n",
    "\n",
    "data = df.dropna()\n",
    "print(data.isna().sum())\n",
    "print(data.shape[0])\n",
    "\n",
    "sns.histplot(data[\"day\"], kde=True, bins=max_day, ax=ax[0, 1])\n",
    "sns.histplot(data[\"rideable_type\"], ax=ax[1, 1])\n",
    "ax[1, 1].set_ylabel(\"\")\n",
    "ax[0, 1].set_ylabel(\"\")\n",
    "ax[0, 1].set_title(\"После\", fontsize=14)\n",
    "plt.suptitle(\"Сравнение данных до\\nи после удаления\", fontsize=16)\n",
    "# plt.savefig(\"Result_of_analisys_nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По графикам видно что ошибки в данных связаны с арендой электровелосипедов и если удалить эти данные мы потеряем львиную долю рынка электровелосепедов и в следствии репрезентативность выборки  \n",
    "**Данные нужно восстанавливать**  \n",
    "Либо удалять равную долю данных по обычным велосипедам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посмотреть на распределение долей между обычными и электро- велосипедами то можно заметить сильный скос (после удаления данных в которых есть пробелы) в сторону обычных велосипедов, а это значит что большинство записей с пропусками это электровелосипеды\n",
    "\n",
    "Допустим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**А почему так происходит?**  \n",
    "Есть довольно простой ответ  \n",
    "\n",
    "Если посмотреть по карте то можно заметить что очень часто велосипеды оставляют просто вне стоянки где попало"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Отдельно стоящие велосипеды](./Отдельные_велосипеды.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зеленое - Стоянки для велосипедов  \n",
    "Красное - электро-велосипеды (ebike)  \n",
    "**Что с этим делать?**  \n",
    "Один из простых вариантов **просто заполнить пропуски меткой что данные велосипеды взяты и припаркованы вне стоянки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({\"start_station_name\": \"droped\", \n",
    "           \"end_station_name\": \"droped\",\n",
    "           \"start_station_id\": \"droped\",\n",
    "           \"end_station_id\": \"droped\"}, inplace=True)\n",
    "\n",
    "df[[\"start_station_name\", \"start_station_id\", \"end_station_name\", \"end_station_id\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Координаты начала и конца поездки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start_lat: широта начальной станции\n",
    "- start_lng: долгота начальной станции\n",
    "- end_lat: широта конечной станции\n",
    "- end_lng: долгота конечной станции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание: разобраться с пропусками в координатах конца поездки**  \n",
    "У нас есть данные с временем аренды больше дня у которых отсутствует точка конца поездки  \n",
    "Это ошибка в данных, но как это можно объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_end_coord = df[df[\"end_lat\"].isna() == True]\n",
    "print(na_end_coord.sort_values(by=\"ride_duration\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.bar(data[\"rideable_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(na_end_coord[\"ride_duration\"].apply(lambda x: x.total_seconds()/3600))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(df[\"ride_duration\"].apply(lambda x: x.total_seconds()/3600))\n",
    "# plt.show()\n",
    "# print(df.sort_values(by=\"ride_duration\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятнее всего это что-то одно либо все сразу:\n",
    "- поездки сотрудников (хотя указано что данные от этого предобработаны)\n",
    "- украденные велосипеды которые куда нибудь укатили и разобрали на запчасти \n",
    "- с велосипедами произошел несчастный случай (велосипед утопили, человека на велосипеде сбили и так далее)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Это мы отфильтровываем и в конечный датасет не берем**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(df[\"ride_duration\"].apply(lambda x: x.total_seconds()/3600))\n",
    "# plt.show()\n",
    "print(df.sort_values(by=\"ride_duration\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"ride_duration\"] > pd.Timedelta(hours=10)].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получшившееся в результате очистки максимальное значение времени поездки очень даже согласовывается с его данными так как это может быть самый обыкновыенный пользователь который взял велосипед покататься на день возле дома, покатался, где то переночевал и вернулся обратно домой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обогащение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = df.groupby([\"start_station_name\"])[[\"start_station_id\", \"start_lat\", \"start_lng\"]].head(5).sort_values(by=\"start_station_id\")\n",
    "# stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"day_of\"] = df[\"started_at\"].apply(lambda x: x.day_name())\n",
    "# df[\"day_of\"].head()\n",
    "df[\"is_hol\"] = df[\"started_at\"].apply(lambda x: x.day_of_week in (5, 6))\n",
    "# df[[\"day_of\", \"is_hol\"]][df[\"is_hol\"]].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос: У нас есть равные и больше даты старта и конца   \n",
    "Это все ошибки произошедшие по тем или иным причинам  \n",
    "Стоит ли это удалять  \n",
    "Это может быть не ошибка в данных, а может быть пользователь просто взял и сразу поставил велосипед**  \n",
    "**UPD**: Это ошибка в нормальной работе системы а значит скорее всего это будет шумом для наших метрик  \n",
    "Проанализируем и удалим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разведочный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{x}_cleared_divvy.csv\", index=False)\n",
    "day.to_csv(f\"{x}_bigger_one_day.csv\", index=False)\n",
    "fix_df.to_csv(f\"{x}_fixing_data.csv\", index=False)\n",
    "na_end_coord.to_csv(f\"{x}_na_end_coord.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"month\"], bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 12})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.histplot(df[df[\"member_casual\"] == \"member\"][\"month\"], bins=12, ax=ax[0])\n",
    "sns.histplot(df[df[\"member_casual\"] == \"casual\"][\"month\"], bins=12, ax=ax[1])\n",
    "ax[0].set_xlabel(\"members\")\n",
    "ax[1].set_xlabel(\"casual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df, x=\"member_casual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
